{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kussy29/machine_learning/blob/main/lab12_nlp_towards_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Is a *queen* really just a *king*, minus a *man*, plus a *woman*?**\n",
        "\n",
        "--------------\n",
        "\n",
        "\n",
        "\n",
        "In class, we dealt with **embeddings** trained for **sentiment classification**. These embeddings are optimized to separate *positive* from *negative* expressions and **do not encode deeper semantic information**.\n",
        "\n",
        "However, in modern natural language processing, there exist other embeddings — such as those from **BERT**, **word2vec**, or **GloVe** — that **do capture semantic structure**. These models are trained on large corpora, and their embeddings often allow for meaningful **vector arithmetic**, like the famous:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
        "```\n",
        "\n",
        "This homework explores **semantic vector relationships** using such pretrained embeddings.\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "1. Construct semantic classes of word pairs.\n",
        "2. Visualize them using PCA.\n",
        "3. Explore arithmetic operations in embedding space.\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "### 1. **Semantic Pair Classes**\n",
        "\n",
        "- You must gather **at least 10 classes** of semantically related word pairs.\n",
        "- Each class must contain **at least 5 pairs**.\n",
        "- That gives a **minimum total of 100 unique words** (10 classes x 5 pairs x 2 words per pair).\n",
        "\n",
        "Two example classes:\n",
        "\n",
        "**Class 1: Gender**\n",
        "\n",
        "- (king, queen)\n",
        "- (man, woman)\n",
        "- (doctor, nurse)\n",
        "- (prince, princess)\n",
        "- *(you must add one more)*\n",
        "\n",
        "**Class 2: Verb tense (past tense)**\n",
        "\n",
        "- (bring, brought)\n",
        "- (get, got)\n",
        "- (like, liked)\n",
        "- *(you must add two more)*\n",
        "\n",
        "**Your job:**\n",
        "\n",
        "- Invent or search for **at least 10 such classes**, including the examples above.\n",
        "- Each class must be conceptually coherent.\n",
        "- Other examples: singular/plural, country/capital, comparative/superlative, tool/user, job/object, etc.\n",
        "\n",
        "### 2. **Global PCA (Across All Words)**\n",
        "\n",
        "- Use PCA to reduce the **entire set of 100 word embeddings** to 2D, and plot it.\n",
        "- Plot the additional **10 separate charts**, one for each class.\n",
        "  - Each chart should display only the 10 words (5 pairs) of the given class.\n",
        "- Points should be labeled with the words themselves.\n",
        "\n",
        "### 3. **Local PCA (Per Class)**\n",
        "\n",
        "- For each class (10 total), perform PCA **only** on the 10 words of that class.\n",
        "- Plot these class-wise PCA visualizations as separate charts.\n",
        "- Again, points should be labeled with the words.\n",
        "\n",
        "**Total: 21 charts**\n",
        "(1 global plot with 100 words + 10 global-space class plots + 10 local PCA class plots)\n",
        "\n",
        "Charts should be presented in a self-explanatory manner with clear labels.\n",
        "\n",
        "### 4. **Embedding Arithmetic**\n",
        "\n",
        "For each class, choose **one example pair** (e.g., (king, queen)) and perform the operation:\n",
        "\n",
        "```\n",
        "embedding(B) - embedding(A) + embedding(C)\n",
        "```\n",
        "\n",
        "Where A and B form a known pair, and C is another base word.\n",
        "For example:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "\n",
        "* For each such result vector, find the **5 closest word embeddings** (using cosine similarity or Euclidean distance).\n",
        "* Print the top 5 neighbors **with their distances**.\n",
        "* Do this **once per class** (i.e., 10 times).\n",
        "\n",
        "This will make it possible to verify if\n",
        " ```\n",
        "embedding(\"queen\") ≈ embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "for the *gender*-related class.\n",
        "\n",
        "\n",
        "### 5. **Discussion**\n",
        "\n",
        "* Analyze and interpret your 21 plots.\n",
        "* Discuss whether the vector relationships are preserved.\n",
        "* Does PCA capture semantic differences?\n",
        "* Are the closest words from the arithmetic meaningful?\n",
        "* What kinds of relationships are captured, and what are not?\n",
        "* Are some classes better behaved than others?\n",
        "\n",
        "\n",
        "### 6. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "*This homework assignment was inspired by an idea from my master's student **Andrzej Małek**, to whom I would like to express my thanks.*\n",
        "\n"
      ],
      "metadata": {
        "id": "_pm7h5fXtWfY"
      },
      "id": "_pm7h5fXtWfY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class 1: Gender**\n",
        "\n",
        "\n",
        "*   King - Queen\n",
        "*   Man - Woman\n",
        "*   Doctor - Nurse\n",
        "*   Prince - Princess\n",
        "*   Actor - Actress\n",
        "*   Uncle - Aunt\n",
        "\n",
        "**Class 2: Past tense**\n",
        "\n",
        "\n",
        "*   bring - brought\n",
        "*   get - got\n",
        "*   like - liked\n",
        "*   giva - gave\n",
        "*   leave - left\n",
        "\n",
        "**Class 3: Capital**\n",
        "\n",
        "*   Poland - Warsaw\n",
        "*   USA - Washington, D.C.\n",
        "*   Washington - Olympia\n",
        "*   Turkey - Ankara\n",
        "*   Liechtenstein - Vaduz\n",
        "\n",
        "**Class 4: Antonims**\n",
        "\n",
        "*   preety - ugly\n",
        "*   happy - sad\n",
        "*   noise - silence\n",
        "*   wealth - poverty\n",
        "*   quickly - slowly\n",
        "*   left - right\n",
        "\n",
        "**Class 5: Adjective**\n",
        "\n",
        "*  Poland - polish\n",
        "*  wealth - wealthy\n",
        "*  happiness - happy\n",
        "*  creativity - creative\n",
        "*  beaty - beatiful\n",
        "\n",
        "**Class 6: Substitutes**\n",
        "\n",
        "*  Orange - tangerine\n",
        "*  Butter - margarine\n",
        "*  Coffee - Tea\n",
        "*  Coca-cola - Pepsi\n",
        "*  Sandwich - Burger\n",
        "\n",
        "**Class 7 - diminutives**\n",
        "\n",
        "*  dear - darling\n",
        "*  Charles - Charlie\n",
        "*  duck - duckling\n",
        "*  dog - doggie\n",
        "*  cat - kitty\n",
        "\n",
        "**Class 8 - Child**\n",
        "\n",
        "*  Dog - puppy\n",
        "*  Cow - calf\n",
        "*  cat - kitten\n",
        "*  Horse - foal\n",
        "*  Swan - cygnet\n",
        "\n",
        "**Class 9 - Meat**\n",
        "\n",
        "*  pig - pork\n",
        "*  cow - beef\n",
        "*  sheep - mutton\n",
        "*  deer - venison\n",
        "*  bird - poultry\n",
        "\n",
        "**Class 10 - Food**\n",
        "\n",
        "*  potato - chips\n",
        "*  lettuce - salad\n",
        "*  pork - chop\n",
        "*  wheat - bread\n",
        "*  avokado - guacamole"
      ],
      "metadata": {
        "id": "j6gDf_edGG4z"
      },
      "id": "j6gDf_edGG4z"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load pretrained Word2Vec model (e.g., Google News)\n",
        "# Make sure you've downloaded it first (e.g., GoogleNews-vectors-negative300.bin)\n",
        "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_dim = word2vec.vector_size\n",
        "vocab_size = len(word2vec.key_to_index)\n",
        "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Mapping from word to index\n",
        "word2idx = {}\n",
        "for i, word in enumerate(word2vec.key_to_index):\n",
        "    embedding_matrix[i] = torch.tensor(word2vec[word])\n",
        "    word2idx[word] = i\n",
        "\n",
        "# Create PyTorch Embedding layer\n",
        "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # freeze=True if not trainable"
      ],
      "metadata": {
        "id": "b0InRGHsm6Aq"
      },
      "id": "b0InRGHsm6Aq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"king\"\n",
        "idx = word2idx[word]\n",
        "embedding = embedding_layer(torch.tensor(idx))"
      ],
      "metadata": {
        "id": "NU6eyt4xnsG2"
      },
      "id": "NU6eyt4xnsG2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}